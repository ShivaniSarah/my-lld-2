Multithreaded Web Scraper with Retry & Throttling
Ensure you do not violate rate limits.
Retry failed requests with exponential backoff.

usimg multithreading and java


ChatGPT said:
Here's a Multithreaded Web Scraper in Java with:

‚úÖ Concurrency using a thread pool
‚úÖ Throttling to respect rate limits
‚úÖ Retry logic with Exponential Backoff for failed requests

üí° Components
ExecutorService for multithreading

Semaphore to throttle concurrent requests

ScheduledExecutorService to manage backoff retries

HttpClient (Java 11+) for HTTP requests

üßµ Java Code Example
java
Copy
Edit
import java.io.IOException;
import java.net.URI;
import java.net.http.*;
import java.time.Duration;
import java.util.List;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicInteger;

public class WebScraper {
    private static final int MAX_CONCURRENT_REQUESTS = 5;
    private static final int MAX_RETRIES = 3;
    private static final long BASE_DELAY_MS = 1000;

    private final ExecutorService executor = Executors.newFixedThreadPool(MAX_CONCURRENT_REQUESTS);
    private final ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(1);
    private final Semaphore throttle = new Semaphore(MAX_CONCURRENT_REQUESTS);
    private final HttpClient httpClient = HttpClient.newHttpClient();

    public void scrape(List<String> urls) {
        for (String url : urls) {
            submitRequest(url, 0);
        }
    }

    private void submitRequest(String url, int attempt) {
        executor.submit(() -> {
            try {
                throttle.acquire();

                HttpRequest request = HttpRequest.newBuilder()
                        .uri(URI.create(url))
                        .timeout(Duration.ofSeconds(5))
                        .GET()
                        .build();

                HttpResponse<String> response = httpClient.send(request, HttpResponse.BodyHandlers.ofString());

                System.out.printf("[SUCCESS] %s (Status: %d)%n", url, response.statusCode());

            } catch (IOException | InterruptedException e) {
                if (attempt < MAX_RETRIES) {
                    long backoff = (long) (BASE_DELAY_MS * Math.pow(2, attempt));
                    System.out.printf("[RETRY %d] %s after %dms%n", attempt + 1, url, backoff);
                    scheduler.schedule(() -> submitRequest(url, attempt + 1), backoff, TimeUnit.MILLISECONDS);
                } else {
                    System.err.printf("[FAILED] %s after %d attempts%n", url, attempt);
                }
            } finally {
                throttle.release();
            }
        });
    }

    public void shutdown() {
        executor.shutdown();
        scheduler.shutdown();
    }

    public static void main(String[] args) {
        WebScraper scraper = new WebScraper();

        List<String> urls = List.of(
                "https://httpbin.org/status/200",
                "https://httpbin.org/status/500",
                "https://httpbin.org/delay/2",
                "https://httpbin.org/status/200"
        );

        scraper.scrape(urls);

        scraper.executor.shutdown();
        try {
            scraper.executor.awaitTermination(1, TimeUnit.MINUTES);
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }

        scraper.shutdown();
    }
}
üîß Features

| Feature             | Implementation                                          |
| ------------------- | ------------------------------------------------------- |
| **Concurrency**     | Fixed thread pool with `Executors.newFixedThreadPool()` |
| **Rate-limiting**   | `Semaphore` limits concurrent executions                |
| **Retries**         | `ScheduledExecutorService` with `exponential backoff`   |
| **Robust Requests** | `HttpClient` with timeout, auto-retry                   |


üí° Output Example
csharp
Copy
Edit
[SUCCESS] https://httpbin.org/status/200 (Status: 200)
[RETRY 1] https://httpbin.org/status/500 after 1000ms
[SUCCESS] https://httpbin.org/status/200 (Status: 200)
[SUCCESS] https://httpbin.org/delay/2 (Status: 200)
[FAILED] https://httpbin.org/status/500 after 3 attempts
üõ†Ô∏è Enhancements
Throttle based on requests per second with a RateLimiter (Guava)

Parse/Extract HTML content (using JSoup)

Save output to file/database

Add proxy support or user-agent rotation

Would you like a version that uses JSoup for HTML parsing or scrapes data to CSV/DB?
